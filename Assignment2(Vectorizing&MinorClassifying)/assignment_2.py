# -*- coding: utf-8 -*-
"""Assignment 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10ZiVVAeUBjJMz-CW_BtdldSY4cccCSmG

##Import
"""

import numpy as np
import pandas as pd
import string

df=pd.read_json('reports.jl',lines=True)

df.head()

"""##Preprocesing/Cleaning"""



def removePunct(txt):
  txt_nopunct="".join([c for c in txt if c not in string.punctuation])
  return txt_nopunct

df['textClean']=df['text'].apply(lambda x:removePunct(x))
df.head()

import re

def tokenize(txt):
  tokens=re.split('\W+',txt)
  return tokens

df['textCleanToken']=df['textClean'].apply(lambda x:tokenize(x.lower()))
df.head()

import nltk
nltk.download('stopwords')
stopwords=nltk.corpus.stopwords.words('english')
stopwords[:10]

def removeStopwords(txt):
  txt_clean=[word for word in txt if word not in stopwords]
  return txt_clean

df['noStopwords']=df['textCleanToken'].apply(lambda x: removeStopwords(x))
df.head()

from nltk.stem import PorterStemmer
ps=PorterStemmer()
def stemming(token_txt):
  text=[ps.stem(word) for word in token_txt]
  return text

df['stemmed']=df['noStopwords'].apply(lambda x: stemming(x))
df.head()

def cleanWords(text):
  txt="".join([p for p in text if p not in string.punctuation])
  tokens=re.split("\W+",txt)
  txt=[ps.stem(word) for word in tokens if word not in stopwords]

df['text']

for i in df['noStopwords'][:5]:
  print(i)

def listToString(s): 
    
    # initialize an empty string
    str1 = " " 
    
    # return string  
    return (str1.join(s))

df['string']=df['stemmed'].apply(lambda x: listToString(x))
df.head()

def removeNum(txt):
  result = ''.join([i for i in txt if not i.isdigit()])
  return result

df['Nonum']=df['string'].apply(lambda x: removeNum(x))

df.head()

"""##CountVector"""

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer()
X=cv.fit_transform(df['Nonum'])
print(X.shape)

df2=pd.DataFrame(X.toarray(),columns=cv.get_feature_names())
df2.head()

scorep1=X.nnz / float(X.shape[0])
scorep1

scorep1/float(X.shape[1])

sum(df2['covid'])

sum(df2['california'])

sum(df2['diseas'])

sum(df2['safe'])

sum(df2['vaccin'])

sum(df2['viral'])

sum(df2['case'])

sum(df2['death'])

"""##TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
tv=TfidfVectorizer()
X=tv.fit_transform(df['Nonum'])
print(X.shape)

df3=pd.DataFrame(X.toarray(),columns=tv.get_feature_names())

df3.head()

scorep1=X.nnz / float(X.shape[0])
scorep1

scorep1/float(X.shape[1])

sum(df3['covid'])

sum(df3['california'])

sum(df3['diseas'])

sum(df3['safe'])

sum(df3['vaccin'])

sum(df3['viral'])

sum(df3['case'])

sum(df3['death'])

"""##Neural Network Embedding"""

df





from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

from nltk.tokenize import word_tokenize

nltk.download('punkt')
data = ["I love machine learning. Its awesome.",
        "I love coding in python",
        "I love building chatbots",
        "they chat amagingly well"]

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data)]

tagged_data

data

df['Nonum']

tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df['Nonum'])]

tagged_data

max_epochs = 100
vec_size = 20
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)
  
model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save("d2v.model")
print("Model Saved")

print(model.docvecs['1'])

len(model.docvecs['5'])

len(model.docvecs['1'])

model.docvecs['5']

similar_doc = model.docvecs.most_similar('1')
print(similar_doc)

similar_doc2 = model.docvecs.most_similar('2')
print(similar_doc2)

df2array=df2.to_numpy()

df2array

df3array=df3.to_numpy()

df3array

df2torecords = df2.to_records(index=False)

df3torecords=df3.to_records(index=False)

"""##RandomForest"""

df

df.index

df['timeframe'] = ['recent' if x <=70 else 'old' for x in df['count']]

df

df2['timeframe'] = ['recent' if x <=70 else 'old' for x in df2.index]
df3['timeframe'] = ['recent' if x <=70 else 'old' for x in df3.index]

df2

df3

"""###Countvector"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(df2.drop(['timeframe'],axis='columns'),df2.timeframe,test_size=0.2)

rfcmodel=RandomForestClassifier()
rfcmodel.fit(x_train,y_train)

rfcmodel.score(x_test,y_test)

"""###TF-IDF"""

x_train, x_test, y_train, y_test = train_test_split(df3.drop(['timeframe'],axis='columns'),df3.timeframe,test_size=0.2)

rfcmodel=RandomForestClassifier()
rfcmodel.fit(x_train,y_train)

rfcmodel.score(x_test,y_test)



"""###NN"""

type(model.docvecs)

nnvec=model.docvecs.vectors_docs

nnvec

nnvec.size

nnvecdf = pd.DataFrame(nnvec)

nnvecdf

nnvecdf['timeframe'] = ['recent' if x <=70 else 'old' for x in nnvecdf.index]

nnvecdf

x_train, x_test, y_train, y_test = train_test_split(nnvecdf.drop(['timeframe'],axis='columns'),nnvecdf.timeframe,test_size=0.2)

rfcmodel=RandomForestClassifier()
rfcmodel.fit(x_train,y_train)

rfcmodel.score(x_test,y_test)

