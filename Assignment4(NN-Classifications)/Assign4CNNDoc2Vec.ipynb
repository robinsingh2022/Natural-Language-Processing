{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assign4CNNDoc2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gwyQ1ry_DIq"
      },
      "source": [
        "##Ingest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4N2GreQLrw4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQIQvg4vMutg"
      },
      "source": [
        "df=pd.read_json('News_Category_Dataset_v2.json',lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8W0bhmM_FCx"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KPJPG2FMwpv"
      },
      "source": [
        "lis=[ 'WORLD NEWS',\n",
        "       'SPORTS', 'MONEY',\n",
        "       'WOMEN','SCIENCE']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX-LZlRyMznl"
      },
      "source": [
        "df=df.loc[df['category'].isin(lis)]\n",
        "df=df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOqlwUOJM3MV"
      },
      "source": [
        "df[\"text\"] = df[\"headline\"] + df[\"short_description\"] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_zlkqYNM7kU"
      },
      "source": [
        "\n",
        "df['text'][1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzIGijCXM9Zs"
      },
      "source": [
        "def removePunct(txt):\n",
        "  txt_nopunct=\"\".join([c for c in txt if c not in string.punctuation])\n",
        "  return txt_nopunct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtPlOOm2NCcU"
      },
      "source": [
        "df['textClean']=df['text'].apply(lambda x:removePunct(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNXL4bgfNFxp"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvSXOTP8NKLV"
      },
      "source": [
        "def tokenize(txt):\n",
        "  tokens=re.split('\\W+',txt)\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9m_CCf5NM9s"
      },
      "source": [
        "df['textCleanToken']=df['textClean'].apply(lambda x:tokenize(x.lower()))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv7ALqnaNOnO"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stopwords=nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhbBWH-fNRBC"
      },
      "source": [
        "def removeStopwords(txt):\n",
        "  txt_clean=[word for word in txt if word not in stopwords]\n",
        "  return txt_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz7M_sMNNVPb"
      },
      "source": [
        "df['noStopwords']=df['textCleanToken'].apply(lambda x: removeStopwords(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30hHhPmUNVu2"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "def stemming(token_txt):\n",
        "  text=[ps.stem(word) for word in token_txt]\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHpptpkLNXnz"
      },
      "source": [
        "df['stemmed']=df['noStopwords'].apply(lambda x: stemming(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY_24DjeNZhF"
      },
      "source": [
        "def cleanWords(text):\n",
        "  txt=\"\".join([p for p in text if p not in string.punctuation])\n",
        "  tokens=re.split(\"\\W+\",txt)\n",
        "  txt=[ps.stem(word) for word in tokens if word not in stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrjV7Lw9NcEH"
      },
      "source": [
        "def listToString(s): \n",
        "    \n",
        "    # initialize an empty string\n",
        "    str1 = \" \" \n",
        "    \n",
        "    # return string  \n",
        "    return (str1.join(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv2YvlmFNeIm"
      },
      "source": [
        "df['string']=df['stemmed'].apply(lambda x: listToString(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcM4jd5SNgLM"
      },
      "source": [
        "def removeNum(txt):\n",
        "  result = ''.join([i for i in txt if not i.isdigit()])\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3EXAEhQNi4Q"
      },
      "source": [
        "df['Nonum']=df['string'].apply(lambda x: removeNum(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbTtFll8V-My"
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csJa9GDGNkZN"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp7KGel9Nl4T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M63NDD2lNvja"
      },
      "source": [
        "##Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZNNbRpZNoD4"
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JskRJFduNvAU"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhqoMefqN10P"
      },
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df['Nonum'])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OIKdII1N4m4"
      },
      "source": [
        "tagged_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcRFRuoGN7A_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e17810d-e1da-46da-a1e1-bf4b4e550834"
      },
      "source": [
        "max_epochs = 100\n",
        "vec_size = 200\n",
        "alpha = 0.025\n",
        "\n",
        "model = Doc2Vec(size=vec_size,\n",
        "                alpha=alpha, \n",
        "                min_alpha=0.00025,\n",
        "                min_count=1,\n",
        "                dm =1)\n",
        "  \n",
        "model.build_vocab(tagged_data)\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "    print('iteration {0}'.format(epoch))\n",
        "    model.train(tagged_data,\n",
        "                total_examples=model.corpus_count,\n",
        "                epochs=model.iter)\n",
        "    # decrease the learning rate\n",
        "    model.alpha -= 0.0002\n",
        "    # fix the learning rate, no decay\n",
        "    model.min_alpha = model.alpha\n",
        "\n",
        "model.save(\"d2v.model\")\n",
        "print(\"Model Saved\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
            "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iteration 1\n",
            "iteration 2\n",
            "iteration 3\n",
            "iteration 4\n",
            "iteration 5\n",
            "iteration 6\n",
            "iteration 7\n",
            "iteration 8\n",
            "iteration 9\n",
            "iteration 10\n",
            "iteration 11\n",
            "iteration 12\n",
            "iteration 13\n",
            "iteration 14\n",
            "iteration 15\n",
            "iteration 16\n",
            "iteration 17\n",
            "iteration 18\n",
            "iteration 19\n",
            "iteration 20\n",
            "iteration 21\n",
            "iteration 22\n",
            "iteration 23\n",
            "iteration 24\n",
            "iteration 25\n",
            "iteration 26\n",
            "iteration 27\n",
            "iteration 28\n",
            "iteration 29\n",
            "iteration 30\n",
            "iteration 31\n",
            "iteration 32\n",
            "iteration 33\n",
            "iteration 34\n",
            "iteration 35\n",
            "iteration 36\n",
            "iteration 37\n",
            "iteration 38\n",
            "iteration 39\n",
            "iteration 40\n",
            "iteration 41\n",
            "iteration 42\n",
            "iteration 43\n",
            "iteration 44\n",
            "iteration 45\n",
            "iteration 46\n",
            "iteration 47\n",
            "iteration 48\n",
            "iteration 49\n",
            "iteration 50\n",
            "iteration 51\n",
            "iteration 52\n",
            "iteration 53\n",
            "iteration 54\n",
            "iteration 55\n",
            "iteration 56\n",
            "iteration 57\n",
            "iteration 58\n",
            "iteration 59\n",
            "iteration 60\n",
            "iteration 61\n",
            "iteration 62\n",
            "iteration 63\n",
            "iteration 64\n",
            "iteration 65\n",
            "iteration 66\n",
            "iteration 67\n",
            "iteration 68\n",
            "iteration 69\n",
            "iteration 70\n",
            "iteration 71\n",
            "iteration 72\n",
            "iteration 73\n",
            "iteration 74\n",
            "iteration 75\n",
            "iteration 76\n",
            "iteration 77\n",
            "iteration 78\n",
            "iteration 79\n",
            "iteration 80\n",
            "iteration 81\n",
            "iteration 82\n",
            "iteration 83\n",
            "iteration 84\n",
            "iteration 85\n",
            "iteration 86\n",
            "iteration 87\n",
            "iteration 88\n",
            "iteration 89\n",
            "iteration 90\n",
            "iteration 91\n",
            "iteration 92\n",
            "iteration 93\n",
            "iteration 94\n",
            "iteration 95\n",
            "iteration 96\n",
            "iteration 97\n",
            "iteration 98\n",
            "iteration 99\n",
            "Model Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBE0NwlHOMkr"
      },
      "source": [
        "##CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWb44gSZN-I2"
      },
      "source": [
        "nnvec=model.docvecs.vectors_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwnG8PmdOOfK"
      },
      "source": [
        "nnvecdf = pd.DataFrame(nnvec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zlyMiQzORiP"
      },
      "source": [
        "y=df['category']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uytp4TWBbhZG"
      },
      "source": [
        "\n",
        "x=nnvecdf.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k_weIfku1Xh"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9toT_mdbhOW"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "yLE = le.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBgF4OC_bhC2"
      },
      "source": [
        "yLE = np.asarray( tf.keras.utils.to_categorical(yLE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkLYyCBSbg3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a07452-06ea-4ef7-86c2-c7b4770f746c"
      },
      "source": [
        "yLE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       ...,\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVl3Z80Obgwa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "trainX, testX, trainy, testy = train_test_split(x, yLE, test_size=0.25, random_state=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xbQ0IW0OWY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fed0a463-5379-400f-df4d-91bc0c42e416"
      },
      "source": [
        "trainX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.09309758,  0.28601527, -0.84262973, ..., -0.42032093,\n",
              "        -0.5559607 ,  0.05260021],\n",
              "       [-0.37408864, -1.3017676 , -0.21571033, ..., -1.3674392 ,\n",
              "         0.67712283,  1.3236877 ],\n",
              "       [-0.16388178,  0.56549436, -1.6666541 , ..., -0.20130236,\n",
              "        -0.50973576,  0.94197243],\n",
              "       ...,\n",
              "       [-0.44317573,  0.27023143, -1.8648895 , ..., -0.48540053,\n",
              "         0.5698723 ,  1.7259253 ],\n",
              "       [-0.04157736,  3.5654485 , -0.3156717 , ..., -0.97866446,\n",
              "        -1.3631786 , -1.1497347 ],\n",
              "       [-0.39830184,  0.61163926, -0.48205402, ..., -0.27936703,\n",
              "        -0.11220853,  0.47539946]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F--TsfpnOfCY"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE5iTQoGbEvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60dd9738-255b-48b2-81e2-906377ed4832"
      },
      "source": [
        "trainX.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10827, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uj7qAT9bGsn"
      },
      "source": [
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqRByzj3fKo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f25b5c97-4e30-4104-aaf9-1e1a06c2ecc7"
      },
      "source": [
        "trainX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.09309758,  0.28601527, -0.84262973, ..., -0.42032093,\n",
              "        -0.5559607 ,  0.05260021],\n",
              "       [-0.37408864, -1.3017676 , -0.21571033, ..., -1.3674392 ,\n",
              "         0.67712283,  1.3236877 ],\n",
              "       [-0.16388178,  0.56549436, -1.6666541 , ..., -0.20130236,\n",
              "        -0.50973576,  0.94197243],\n",
              "       ...,\n",
              "       [-0.44317573,  0.27023143, -1.8648895 , ..., -0.48540053,\n",
              "         0.5698723 ,  1.7259253 ],\n",
              "       [-0.04157736,  3.5654485 , -0.3156717 , ..., -0.97866446,\n",
              "        -1.3631786 , -1.1497347 ],\n",
              "       [-0.39830184,  0.61163926, -0.48205402, ..., -0.27936703,\n",
              "        -0.11220853,  0.47539946]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6a8R2s3hDCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a08303-034f-4a88-ce71-26fa7ce02eb5"
      },
      "source": [
        "import numpy as np\n",
        "np.amin(trainX)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-5.2274995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Zdta01icPW"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "# transform data\n",
        "trainX = scaler.fit_transform(trainX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI7LDlXZiia5"
      },
      "source": [
        "testX = scaler.fit_transform(testX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG9DuSDLbM4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582ad9fd-1939-4bb6-ca12-d393ba37005a"
      },
      "source": [
        "max_features =200\n",
        "embedding_dim =200\n",
        "sequence_length = 201\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(max_features +1, embedding_dim, \n",
        "                                    embeddings_regularizer = regularizers.l2(0.0005)))                                    \n",
        "\n",
        "model.add(tf.keras.layers.Conv1D(360,3, activation='relu',\\\n",
        "                                 kernel_regularizer = regularizers.l2(0.0005),\\\n",
        "                                 bias_regularizer = regularizers.l2(0.0005)))                               \n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(5, activation='sigmoid',\\\n",
        "                                kernel_regularizer=regularizers.l2(0.001),\\\n",
        "                                bias_regularizer=regularizers.l2(0.001),))\n",
        "                               \n",
        "\n",
        "\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), optimizer='Nadam', metrics=[\"CategoricalAccuracy\"])"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 200)         40200     \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 360)         216360    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 360)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 360)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 1805      \n",
            "=================================================================\n",
            "Total params: 258,365\n",
            "Trainable params: 258,365\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qJS_-cUbW1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a24e371c-8b36-4fe1-dd4e-97bc3708e95f"
      },
      "source": [
        "epochs = 20\n",
        "# Fit the model using the train and test datasets.\n",
        "history = model.fit(trainX, trainy,validation_data= (testX,testy),epochs=epochs )"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5342 - categorical_accuracy: 0.3403 - val_loss: 1.5381 - val_categorical_accuracy: 0.3325\n",
            "Epoch 2/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5343 - categorical_accuracy: 0.3403 - val_loss: 1.5375 - val_categorical_accuracy: 0.3325\n",
            "Epoch 3/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5344 - categorical_accuracy: 0.3403 - val_loss: 1.5368 - val_categorical_accuracy: 0.3325\n",
            "Epoch 4/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5339 - categorical_accuracy: 0.3403 - val_loss: 1.5368 - val_categorical_accuracy: 0.3325\n",
            "Epoch 5/20\n",
            "339/339 [==============================] - 44s 130ms/step - loss: 1.5342 - categorical_accuracy: 0.3403 - val_loss: 1.5372 - val_categorical_accuracy: 0.3325\n",
            "Epoch 6/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5344 - categorical_accuracy: 0.3403 - val_loss: 1.5370 - val_categorical_accuracy: 0.3325\n",
            "Epoch 7/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5338 - categorical_accuracy: 0.3403 - val_loss: 1.5365 - val_categorical_accuracy: 0.3325\n",
            "Epoch 8/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5345 - categorical_accuracy: 0.3403 - val_loss: 1.5365 - val_categorical_accuracy: 0.3325\n",
            "Epoch 9/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5340 - categorical_accuracy: 0.3403 - val_loss: 1.5372 - val_categorical_accuracy: 0.3325\n",
            "Epoch 10/20\n",
            "339/339 [==============================] - 44s 130ms/step - loss: 1.5338 - categorical_accuracy: 0.3403 - val_loss: 1.5386 - val_categorical_accuracy: 0.3325\n",
            "Epoch 11/20\n",
            "339/339 [==============================] - 44s 130ms/step - loss: 1.5342 - categorical_accuracy: 0.3403 - val_loss: 1.5380 - val_categorical_accuracy: 0.3325\n",
            "Epoch 12/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5335 - categorical_accuracy: 0.3403 - val_loss: 1.5370 - val_categorical_accuracy: 0.3325\n",
            "Epoch 13/20\n",
            "339/339 [==============================] - 44s 130ms/step - loss: 1.5341 - categorical_accuracy: 0.3403 - val_loss: 1.5377 - val_categorical_accuracy: 0.3325\n",
            "Epoch 14/20\n",
            "339/339 [==============================] - 44s 130ms/step - loss: 1.5337 - categorical_accuracy: 0.3403 - val_loss: 1.5368 - val_categorical_accuracy: 0.3325\n",
            "Epoch 15/20\n",
            "339/339 [==============================] - 44s 130ms/step - loss: 1.5337 - categorical_accuracy: 0.3403 - val_loss: 1.5381 - val_categorical_accuracy: 0.3325\n",
            "Epoch 16/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5336 - categorical_accuracy: 0.3403 - val_loss: 1.5373 - val_categorical_accuracy: 0.3325\n",
            "Epoch 17/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5343 - categorical_accuracy: 0.3403 - val_loss: 1.5371 - val_categorical_accuracy: 0.3325\n",
            "Epoch 18/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5333 - categorical_accuracy: 0.3403 - val_loss: 1.5370 - val_categorical_accuracy: 0.3325\n",
            "Epoch 19/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5336 - categorical_accuracy: 0.3403 - val_loss: 1.5367 - val_categorical_accuracy: 0.3325\n",
            "Epoch 20/20\n",
            "339/339 [==============================] - 44s 131ms/step - loss: 1.5337 - categorical_accuracy: 0.3403 - val_loss: 1.5371 - val_categorical_accuracy: 0.3325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu6ifUa-LmYx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}