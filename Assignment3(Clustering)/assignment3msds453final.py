# -*- coding: utf-8 -*-
"""Assignment3MSDS453Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pNGqRMZtiJW7QSkq8FdYuHf9Iuh63M1x

##Ingest
"""

import numpy as np
import pandas as pd
import string

df=pd.read_json('News_Category_Dataset_v2.json',lines=True)

df

df=df.iloc[0:3000]

df

lis=['CRIME', 'ENTERTAINMENT', 'WORLD NEWS', 'POLITICS',
       'SPORTS', 'BUSINESS', 'TRAVEL', 'MEDIA', 'TECH',
       'SCIENCE']

df['category'].unique()

df=df.loc[df['category'].isin(lis)]

df=df.reset_index(drop=True)

df

def removePunct(txt):
  txt_nopunct="".join([c for c in txt if c not in string.punctuation])
  return txt_nopunct

df['textClean']=df['short_description'].apply(lambda x:removePunct(x))
df.head()

import re

def tokenize(txt):
  tokens=re.split('\W+',txt)
  return tokens

df['textCleanToken']=df['textClean'].apply(lambda x:tokenize(x.lower()))
df.head()

import nltk
nltk.download('stopwords')
stopwords=nltk.corpus.stopwords.words('english')

def removeStopwords(txt):
  txt_clean=[word for word in txt if word not in stopwords]
  return txt_clean

df['noStopwords']=df['textCleanToken'].apply(lambda x: removeStopwords(x))
df.head()

from nltk.stem import PorterStemmer
ps=PorterStemmer()
def stemming(token_txt):
  text=[ps.stem(word) for word in token_txt]
  return text

df['stemmed']=df['noStopwords'].apply(lambda x: stemming(x))
df.head()

def cleanWords(text):
  txt="".join([p for p in text if p not in string.punctuation])
  tokens=re.split("\W+",txt)
  txt=[ps.stem(word) for word in tokens if word not in stopwords]

def listToString(s): 
    
    # initialize an empty string
    str1 = " " 
    
    # return string  
    return (str1.join(s))

df['string']=df['stemmed'].apply(lambda x: listToString(x))
df.head()

def removeNum(txt):
  result = ''.join([i for i in txt if not i.isdigit()])
  return result

df['Nonum']=df['string'].apply(lambda x: removeNum(x))

df.head()

"""##TF-IDF Reduced Matrix"""

from sklearn.feature_extraction.text import TfidfVectorizer
tv=TfidfVectorizer()
X=tv.fit_transform(df['Nonum'])
print(X.shape)

df3=pd.DataFrame(X.toarray(),columns=tv.get_feature_names())

df3.head()

"""##K-Means Cluster"""

from matplotlib import pyplot as plt
from sklearn.cluster import KMeans

k = 10
model = KMeans(n_clusters=k, init='k-means++', max_iter=2273, n_init=15,random_state=20)
model.fit(X)

df['clusters']=model.labels_

X

df

model.labels_

max(model.labels_)

for i in range(0,10):
  print(df.loc[df['clusters'] == i])

model.inertia_

centroids=model.cluster_centers_

from sklearn.decomposition import PCA

predictions=model.predict(df3)

predictions

len(predictions)

pca=PCA(n_components=2).fit(df3)

centroids=pca.transform(centroids)

pcadata=pca.transform(df3)

pcadata

color1=["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf"]
color=[color1[i] for i in predictions]
plt.scatter(pcadata[:,0],pcadata[:,1],c=color)
plt.scatter(centroids[:,0],centroids[:,1],marker="D",s=50,c='#000000')
plt.show()









data = pd.DataFrame({"Index": df.index, "Cluster": df.clusters, "Category": df.category})

data

groups = data.groupby("Category")
for name, group in groups:
    plt.plot(group["Index"], group["Cluster"], marker="o", linestyle="", label=name)
plt.legend()

from sklearn.manifold import TSNE

model=TSNE(learning_rate=100)

tsneFeats=model.fit_transform(X)

tsneFeats[1:4,:]

import seaborn as sns

sns.scatterplot(x=tsneFeats[:,0],y=tsneFeats[:,1])

data['tsneX']=tsneFeats[:,0]
data['tsneY']=tsneFeats[:,1]

sns.scatterplot(x='tsneX',y='tsneY',hue='Category', data=data)

sns.scatterplot(x='tsneX',y='tsneY',hue='Cluster', data=data)

import scipy.cluster.hierarchy as sch

from sklearn.cluster import AgglomerativeClustering

dendrogram=sch.dendrogram(sch.linkage(df3,method='ward'))

heiCluster=AgglomerativeClustering(n_clusters=10, affinity='euclidean',linkage='ward')

heirachyFit=heiCluster.fit_predict(df3)

heirachyFit

max(heirachyFit)

data['heirarchy']=heirachyFit

data

sns.scatterplot(x='Index',y='heirarchy',hue='Cluster', data=data)

sns.scatterplot(x='Index',y='heirarchy',hue='Category', data=data)

color1=["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd", "#8c564b", "#e377c2", "#7f7f7f", "#bcbd22", "#17becf"]
color=[color1[i] for i in heirachyFit]
plt.scatter(pcadata[:,0],pcadata[:,1],c=color)
plt.scatter(centroids[:,0],centroids[:,1],marker="D",s=50,c='#000000')
plt.show()









"""##LDA topic modelling"""

import sklearn.metrics as sm

from sklearn.decomposition import LatentDirichletAllocation

X = df3.values

lda = LatentDirichletAllocation(n_components=10,random_state=0)

lda.fit(X)

lda.transform(df3)

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')



dictionary = gensim.corpora.Dictionary(df.stemmed)
count = 0
for k, v in dictionary.iteritems():
    print(k, v)
    count += 1
    if count > 10:
        break

dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)

bow_corpus = [dictionary.doc2bow(doc) for doc in df.stemmed]
bow_corpus[1999]

bow_doc_4310 = bow_corpus[1999]
for i in range(len(bow_doc_4310)):
    print("Word {} (\"{}\") appears {} time.".format(bow_doc_4310[i][0], 
                                               dictionary[bow_doc_4310[i][0]], 
bow_doc_4310[i][1]))

from gensim import corpora, models
tfidf = models.TfidfModel(bow_corpus)
corpus_tfidf = tfidf[bow_corpus]
from pprint import pprint
for doc in corpus_tfidf:
    pprint(doc)
    break

lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)

for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)
for idx, topic in lda_model_tfidf.print_topics(-1):
    print('Topic: {} Word: {}'.format(idx, topic))

df['stemmed'][1999]

for index, score in sorted(lda_model[bow_corpus[1999]], key=lambda tup: -1*tup[1]):
    print("\nScore: {}\t \nTopic: {}".format(score, lda_model.print_topic(index, 10)))

for index, score in sorted(lda_model_tfidf[bow_corpus[1999]], key=lambda tup: -1*tup[1]):
    print("\nScore: {}\t \nTopic: {}".format(score, lda_model_tfidf.print_topic(index, 10)))

import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
import numpy as np
np.random.seed(2018)
import nltk
nltk.download('wordnet')

stemmer = SnowballStemmer('english')
def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
def preprocess(text):
    result = []
    for token in gensim.utils.simple_preprocess(text):
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
    return result

unseen_document = 'Trump talks to Obama'
bow_vector = dictionary.doc2bow(preprocess(unseen_document))
for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):
    print("Score: {}\t Topic: {}".format(score, lda_model.print_topic(index, 5)))

"""##Biclustering Doc2Vec"""

from gensim.test.utils import common_texts
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize

import nltk


nltk.download('punkt')
tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(df['Nonum'])]

max_epochs = 100
vec_size = 200
alpha = 0.025

model = Doc2Vec(size=vec_size,
                alpha=alpha, 
                min_alpha=0.00025,
                min_count=1,
                dm =1)
  
model.build_vocab(tagged_data)

for epoch in range(max_epochs):
    print('iteration {0}'.format(epoch))
    model.train(tagged_data,
                total_examples=model.corpus_count,
                epochs=model.iter)
    # decrease the learning rate
    model.alpha -= 0.0002
    # fix the learning rate, no decay
    model.min_alpha = model.alpha

model.save("d2v.model")
print("Model Saved")

nnvec=model.docvecs.vectors_docs
from sklearn.cluster import SpectralBiclustering
from sklearn.metrics import consensus_score




plt.matshow(nnvec, cmap=plt.cm.Blues)
plt.title("Original dataset")

rng = np.random.RandomState(0)
row_idx = rng.permutation(nnvec.shape[0])
col_idx = rng.permutation(nnvec.shape[1])
data = nnvec[row_idx][:, col_idx]

plt.matshow(data, cmap=plt.cm.Blues)
plt.title("Shuffled dataset")

model = SpectralBiclustering(n_clusters=10, method='log',
                             random_state=0)
model.fit(data)

fit_data = data[np.argsort(model.row_labels_)]
fit_data = fit_data[:, np.argsort(model.column_labels_)]

plt.matshow(fit_data, cmap=plt.cm.Blues)
plt.title("After biclustering; rearranged to show biclusters")

plt.matshow(np.outer(np.sort(model.row_labels_) + 1,
                     np.sort(model.column_labels_) + 1),
            cmap=plt.cm.Blues)
plt.title("Checkerboard structure of rearranged data")

plt.show()

